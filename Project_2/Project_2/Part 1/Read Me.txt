Software used - VMWare Work Station 7.1.3
Hadoop version - 0.20.1
Cloudera Version - 0.3.3

              
Observation from Part I :
		I learnt about how the Map/Reduce framework works using a key, value pair.
		A key/value pair was given as the input to the Reducer.
		This key value pair was generated by the mapper class with word as the key and the count as the value.
		This specific program also has a combiner tht does local aggregation of the key/value pair.
		The reducer class does the job of summing up the values for each key for this specific program using the reducer method
		The final output is also a key/value pair of the word total occurences.
		
		In General the mapper class, changes the given input to a specific format that can be passed to the reducer to be reduced based on the input key
		The reduced occurence is also in a specific format which is then saved to the output file given during run
		The run has the various job configuration and the input and output files to be used and generated.
		
		As was discussed, there will be high fault tolerance and high throughput for companies and organisations that deal with large datasets through hadoop